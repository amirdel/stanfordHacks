{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from keras.layers import merge, Input, Conv2D, MaxPooling2D, UpSampling2D, Cropping2D, Flatten, Dense, BatchNormalization, Dropout\n",
    "from keras.models import Model\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.optimizers import Adam\n",
    "import os\n",
    "from keras.backend import binary_crossentropy\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth = 1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "backend = tf\n",
    "\n",
    "def focal(alpha=0.25, gamma=5.0):\n",
    "    def _focal(y_true, y_pred):\n",
    "        labels         = y_true\n",
    "        classification = y_pred\n",
    "\n",
    "        # compute the focal loss\n",
    "        alpha_factor = keras.backend.ones_like(labels) * alpha\n",
    "        alpha_factor = backend.where(keras.backend.equal(labels, 1), alpha_factor, 1 - alpha_factor)\n",
    "        focal_weight = backend.where(keras.backend.equal(labels, 1), 1 - classification, classification)\n",
    "        focal_weight = alpha_factor * focal_weight ** gamma\n",
    "\n",
    "        cls_loss = focal_weight * keras.backend.binary_crossentropy(labels, classification)\n",
    "\n",
    "#         # filter out \"ignore\" anchors\n",
    "#         anchor_state = keras.backend.max(labels, axis=2)  # -1 for ignore, 0 for background, 1 for object\n",
    "#         indices      = backend.where(keras.backend.not_equal(anchor_state, -1))\n",
    "#         cls_loss     = backend.gather_nd(cls_loss, indices)\n",
    "\n",
    "#         # compute the normalizer: the number of positive anchors\n",
    "#         normalizer = backend.where(keras.backend.equal(anchor_state, 1))\n",
    "#         normalizer = keras.backend.cast(keras.backend.shape(normalizer)[0], keras.backend.floatx())\n",
    "#         normalizer = keras.backend.maximum(1.0, normalizer)\n",
    "\n",
    "#         return keras.backend.sum(cls_loss) / normalizer\n",
    "        return cls_loss\n",
    "\n",
    "    return _focal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_coef(y_true, y_pred):\n",
    "    intersection = K.sum(y_true * y_pred, axis=[0, -1, -2])\n",
    "    sum_ = K.sum(y_true + y_pred, axis=[0, -1, -2])\n",
    "\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "\n",
    "    return K.mean(jac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_coef_int(y_true, y_pred):\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "\n",
    "    intersection = K.sum(y_true * y_pred_pos, axis=[0, -1, -2])\n",
    "    sum_ = K.sum(y_true + y_pred_pos, axis=[0, -1, -2])\n",
    "\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "\n",
    "    return K.mean(jac)\n",
    "\n",
    "\n",
    "def jaccard_coef_loss(y_true, y_pred):\n",
    "    return -K.log(jaccard_coef(y_true, y_pred)) + binary_crossentropy(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crop_shape(target, refer):\n",
    "        # width, the 3rd dimension\n",
    "        cw = (target.get_shape()[2] - refer.get_shape()[2]).value\n",
    "        assert (cw >= 0)\n",
    "        if cw % 2 != 0:\n",
    "            cw1, cw2 = int(cw/2), int(cw/2) + 1\n",
    "        else:\n",
    "            cw1, cw2 = int(cw/2), int(cw/2)\n",
    "        # height, the 2nd dimension\n",
    "        ch = (target.get_shape()[1] - refer.get_shape()[1]).value\n",
    "        assert (ch >= 0)\n",
    "        if ch % 2 != 0:\n",
    "            ch1, ch2 = int(ch/2), int(ch/2) + 1\n",
    "        else:\n",
    "            ch1, ch2 = int(ch/2), int(ch/2)\n",
    "\n",
    "        return (ch1, ch2), (cw1, cw2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unet(num_channels, img_rows, img_cols):\n",
    "    \n",
    "    inputs = Input((img_rows, img_cols, num_channels))\n",
    "    conv1 = Conv2D(32, 3, padding='same', kernel_initializer='he_uniform', activation = 'elu')(inputs)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "#     conv1 = BatchNormalization(mode=0, axis=1)(conv1)\n",
    "#     conv1 = keras.layers.advanced_activations.ELU()(conv1)\n",
    "    conv1 = Conv2D(32, 3, padding='same', kernel_initializer='he_uniform', activation = 'elu')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "#     conv1 = BatchNormalization(mode=0, axis=1)(conv1)\n",
    "#     conv1 = keras.layers.advanced_activations.ELU()(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    print pool1.shape\n",
    "    conv2 = Conv2D(64, 3, padding='same', kernel_initializer='he_uniform', activation = 'elu')(pool1)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    # conv2 = BatchNormalization(mode=0, axis=1)(conv2)\n",
    "    # conv2 = keras.layers.advanced_activations.ELU()(conv2)\n",
    "    conv2 = Conv2D(64, 3, padding='same', kernel_initializer='he_uniform', activation = 'elu')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    # conv2 = BatchNormalization(mode=0, axis=1)(conv2)\n",
    "    # conv2 = keras.layers.advanced_activations.ELU()(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(128, 3, padding='same', kernel_initializer='he_uniform', activation = 'elu')(pool2)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    # conv3 = BatchNormalization(mode=0, axis=1)(conv3)\n",
    "    # conv3 = keras.layers.advanced_activations.ELU()(conv3)\n",
    "    conv3 = Conv2D(128, 3, padding='same', kernel_initializer='he_uniform', activation = 'elu')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    # conv3 = BatchNormalization(mode=0, axis=1)(conv3)\n",
    "    # conv3 = keras.layers.advanced_activations.ELU()(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(256, 3, padding='same', kernel_initializer ='he_uniform', activation = 'elu')(pool3)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    # conv4 = BatchNormalization(mode=0, axis=1)(conv4)\n",
    "    # conv4 = keras.layers.advanced_activations.ELU()(conv4)\n",
    "    conv4 = Conv2D(256, 3, padding='same', kernel_initializer ='he_uniform', activation = 'elu')(conv4)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    # conv4 = BatchNormalization(mode=0, axis=1)(conv4)\n",
    "    # conv4 = keras.layers.advanced_activations.ELU()(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    \n",
    "    conv5 = Conv2D(512, 3, padding='same', kernel_initializer='he_uniform', activation = 'elu')(pool4)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    # conv5 = BatchNormalization(mode=0, axis=1)(conv5)\n",
    "    # conv5 = keras.layers.advanced_activations.ELU()(conv5)\n",
    "    conv5 = Conv2D(512, 3, padding='same', kernel_initializer='he_uniform', activation = 'elu')(conv5)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    # conv5 = BatchNormalization(mode=0, axis=1)(conv5)\n",
    "    # conv5 = keras.layers.advanced_activations.ELU()(conv5)\n",
    "\n",
    "    up6 = merge([UpSampling2D(size=(2, 2))(conv5), conv4], mode='concat', concat_axis=3)\n",
    "    conv6 = Conv2D(256, 3, padding='same', kernel_initializer='he_uniform', activation = 'elu')(up6)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    # conv6 = BatchNormalization(mode=0, axis=1)(conv6)\n",
    "    # conv6 = keras.layers.advanced_activations.ELU()(conv6)\n",
    "    conv6 = Conv2D(256, 3, padding='same', kernel_initializer='he_uniform', activation = 'elu')(conv6)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    # conv6 = BatchNormalization(mode=0, axis=1)(conv6)\n",
    "    # conv6 = keras.layers.advanced_activations.ELU()(conv6)\n",
    "\n",
    "    up7 = merge([UpSampling2D(size=(2, 2))(conv6), conv3], mode='concat', concat_axis=3)\n",
    "    conv7 = Conv2D(128, 3, padding='same', kernel_initializer='he_uniform', activation = 'elu')(up7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    # conv7 = BatchNormalization(mode=0, axis=1)(conv7)\n",
    "    # conv7 = keras.layers.advanced_activations.ELU()(conv7)\n",
    "    conv7 = Conv2D(128, 3, padding='same', kernel_initializer='he_uniform', activation = 'elu')(conv7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    # conv7 = BatchNormalization(mode=0, axis=1)(conv7)\n",
    "    # conv7 = keras.layers.advanced_activations.ELU()(conv7)\n",
    "\n",
    "    up8 = merge([UpSampling2D(size=(2, 2))(conv7), conv2], mode='concat', concat_axis=3)\n",
    "    conv8 = Conv2D(64, 3, padding='same', kernel_initializer='he_uniform', activation = 'elu')(up8)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "#     conv8 = BatchNormalization(mode=0, axis=1)(conv8)\n",
    "#     conv8 = keras.layers.advanced_activations.ELU()(conv8)\n",
    "    conv8 = Conv2D(64, 3, padding='same', kernel_initializer='he_uniform', activation = 'elu')(conv8)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "#     conv8 = BatchNormalization(mode=0, axis=1)(conv8)\n",
    "#     conv8 = keras.layers.advanced_activations.ELU()(conv8)\n",
    "\n",
    "    up9 = merge([UpSampling2D(size=(2, 2))(conv8), conv1], mode='concat', concat_axis=3)\n",
    "    conv9 = Conv2D(32, 3, padding='same', kernel_initializer='he_uniform', activation = 'elu')(up9)\n",
    "    conv9 = BatchNormalization()(conv9)\n",
    "#     conv9 = BatchNormalization(mode=0, axis=1)(conv9)\n",
    "#     conv9 = keras.layers.advanced_activations.ELU()(conv9)\n",
    "    conv9 = Conv2D(32, 3, padding='same', kernel_initializer='he_uniform', activation = 'elu')(conv9)\n",
    "    crop9 = Cropping2D(cropping=((16, 16), (16, 16)))(conv9)\n",
    "    conv9 = BatchNormalization()(conv9)\n",
    "#     conv9 = BatchNormalization(mode=0, axis=1)(crop9)\n",
    "#     conv9 = keras.layers.advanced_activations.ELU()(conv9)\n",
    "    conv10 = Conv2D(1, 1, activation='sigmoid')(conv9)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[conv10])\n",
    "    adam = Adam(lr=1e-3)\n",
    "    model.compile(adam, loss=jaccard_coef_loss, metrics=['binary_crossentropy', jaccard_coef_int])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 128, 128, 32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:53: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/local/lib/python2.7/dist-packages/keras/legacy/layers.py:465: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:63: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:73: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:83: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    }
   ],
   "source": [
    "model = get_unet(3, img_size, img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgs_train = [np.expand_dims(np.array(Image.open('/root/data/hackathon/building_massa_dataset/images/train/22678915_15.tiff')), axis=0)]\n",
    "# imgs_mask_train = [np.expand_dims(np.array(Image.open('/root/data/hackathon/building_massa_dataset/labels/train/22678915_15.tif'))[6:-6, 6:-6], axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = 10000//batch_size\n",
    "val_steps = 1000//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myGenerator(file_paths, steps_per_epoch, BATCH_SIZE, INPUT_SHAPE):\n",
    "    i = 0\n",
    "    while True:\n",
    "        x_batch = np.empty((BATCH_SIZE, INPUT_SHAPE[0], INPUT_SHAPE[1], INPUT_SHAPE[2]))\n",
    "        y_batch = np.empty((BATCH_SIZE, INPUT_SHAPE[0], INPUT_SHAPE[1], 1))\n",
    "        for (ind, j) in enumerate(range(i*BATCH_SIZE, (i+1)*BATCH_SIZE)):\n",
    "            # pick a random image\n",
    "            f = np.random.choice(file_paths)\n",
    "            random_x = np.random.randint(0, 1500-img_size)\n",
    "            random_y = np.random.randint(0, 1500-img_size)\n",
    "            xb = np.array(Image.open(f))[random_x:random_x+img_size, random_y:random_y+img_size, :]\n",
    "            ftruth = f.replace('images', 'labels')\n",
    "            ftruth = ftruth[:-1]\n",
    "            yb = np.expand_dims(np.array(Image.open(ftruth))[random_x:random_x+img_size, random_y:random_y+img_size, 0], axis=2)\n",
    "            yb[yb==255]=1\n",
    "            \n",
    "            if np.random.random() < 0.5:\n",
    "                xb = flip_axis(xb, 1)\n",
    "                yb = flip_axis(yb, 1)\n",
    "            if np.random.random() < 0.5:\n",
    "                xb = flip_axis(xb, 2)\n",
    "                yb = flip_axis(yb, 2)\n",
    "            if np.random.random() < 0.5:\n",
    "                xb = xb.swapaxes(1, 2)\n",
    "                yb = yb.swapaxes(1, 2)\n",
    "                \n",
    "            x_batch[ind,...] = xb\n",
    "            y_batch[ind,...] = yb\n",
    "        # bunch of augmentation\n",
    "        seq = iaa.Sequential([iaa.Sometimes(0.5, iaa.GaussianBlur(sigma=(0, 2.0))),\n",
    "                              iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)),\n",
    "                              iaa.ContrastNormalization((0.75, 1.2)),\n",
    "                              iaa.ContrastNormalization((0.5, 1.0))],\n",
    "                             random_order=True)\n",
    "        x_batch = seq.augment_images(x_batch)\n",
    "        i += 1\n",
    "        if i >= steps_per_epoch:\n",
    "            i = 0\n",
    "        yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [os.path.join('/root/data/hackathon/building_massa_dataset/images/train/', f ) for f in os.listdir('/root/data/hackathon/building_massa_dataset/images/train/') if f.endswith('.tiff')]\n",
    "val = [os.path.join('/root/data/hackathon/building_massa_dataset/images/valid/', f ) for f in os.listdir('/root/data/hackathon/building_massa_dataset/images/valid/') if f.endswith('.tiff')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = myGenerator(train, train_steps, batch_size, (img_size, img_size, 3))\n",
    "validation_generator = myGenerator(val, val_steps, batch_size, (img_size, img_size, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_jaccard_coef_int',\n",
    "                              factor=0.5, \n",
    "                              patience=1, \n",
    "                              min_lr=1e-6)\n",
    "checkpoint = ModelCheckpoint('/root/data/hackathon/weights_{epoch:02d}.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [reduce_lr, checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "625/625 [==============================] - 554s 887ms/step - loss: 2.4488 - binary_crossentropy: 0.7755 - jaccard_coef_int: 0.4828 - val_loss: 1.6882 - val_binary_crossentropy: 0.5952 - val_jaccard_coef_int: 0.5208\n",
      "Epoch 2/50\n",
      "625/625 [==============================] - 549s 878ms/step - loss: 1.7610 - binary_crossentropy: 0.7095 - jaccard_coef_int: 0.5507 - val_loss: 1.5236 - val_binary_crossentropy: 0.6327 - val_jaccard_coef_int: 0.5728\n",
      "Epoch 3/50\n",
      "625/625 [==============================] - 549s 878ms/step - loss: 1.5758 - binary_crossentropy: 0.7000 - jaccard_coef_int: 0.5898 - val_loss: 1.5733 - val_binary_crossentropy: 0.7500 - val_jaccard_coef_int: 0.5686\n",
      "Epoch 4/50\n",
      "625/625 [==============================] - 549s 878ms/step - loss: 1.5436 - binary_crossentropy: 0.7222 - jaccard_coef_int: 0.5916 - val_loss: 1.3278 - val_binary_crossentropy: 0.6347 - val_jaccard_coef_int: 0.6121\n",
      "Epoch 5/50\n",
      "625/625 [==============================] - 549s 878ms/step - loss: 1.4753 - binary_crossentropy: 0.7237 - jaccard_coef_int: 0.6054 - val_loss: 1.2663 - val_binary_crossentropy: 0.6841 - val_jaccard_coef_int: 0.6314\n",
      "Epoch 6/50\n",
      "625/625 [==============================] - 549s 878ms/step - loss: 1.4300 - binary_crossentropy: 0.7277 - jaccard_coef_int: 0.6164 - val_loss: 1.2420 - val_binary_crossentropy: 0.6391 - val_jaccard_coef_int: 0.6292\n",
      "Epoch 7/50\n",
      "625/625 [==============================] - 549s 879ms/step - loss: 1.3750 - binary_crossentropy: 0.7308 - jaccard_coef_int: 0.6285 - val_loss: 1.2568 - val_binary_crossentropy: 0.6907 - val_jaccard_coef_int: 0.6419\n",
      "Epoch 8/50\n",
      "119/625 [====>.........................] - ETA: 6:57 - loss: 1.3693 - binary_crossentropy: 0.7300 - jaccard_coef_int: 0.6292"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "history = model.fit_generator(\n",
    "        generator=train_generator,\n",
    "        steps_per_epoch=train_steps,\n",
    "        epochs=50,\n",
    "        verbose=1,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=val_steps,\n",
    "        callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit_generator(imgs_train, \n",
    "#                     imgs_mask_train, \n",
    "#                     batch_size=4, \n",
    "#                     nb_epoch=10, \n",
    "#                     verbose=1,\n",
    "#                     validation_split=0.2, \n",
    "#                     shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = model.predict(np.expand_dims(np.array(Image.open(train[0]))[:256, :256, :], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.where(test>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "truc = test[0,...,0]\n",
    "plt.imshow(truc)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tru = test[0,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tru[tru>0.5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
